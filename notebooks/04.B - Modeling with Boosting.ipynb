{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook will use Gradient Boosting to classify the Reddit data into the correct subreddit. First, the joined data will be split into train and test data, then preprocessed with Latent Semantic Analysis for dimensionality reduction, and then have the Gradient Boosting model applied._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining, Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1725, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the dataset\n",
    "join = pd.read_csv('./datasets/joined.csv')\n",
    "join.head()\n",
    "\n",
    "# Assigning target\n",
    "target = join['subreddit']\n",
    "\n",
    "# Dropping selftext and title columns\n",
    "df = join.drop(['selftext','title'], axis=1)\n",
    "\n",
    "# Importing nltk's English stop words\n",
    "# Note: Using CamelCase to prevent overwriting variable later\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "# From the EDA, I concluded that the following words were very common \n",
    "# in both /r/Jokes and /r/AntiJokes and were added noise. \n",
    "# I added these words to the nltk stop words\n",
    "stopWords.extend(['wa', 'say', 'said', 'did', 'like', 'asked', 'woman', \n",
    "                  'don', 'know', 'year', 'wife', 'good', 'want', 'got', \n",
    "                  'ha', 'people', 'make', 'tell', 'didn', 'joke', 'x200b', \n",
    "                  'way', 'think', 'walk', 'll', 'home', 't'])\n",
    "\n",
    "# Tokenizing by alphanumeric characters\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "\n",
    "# Making all tokens lowercase\n",
    "tokens = [tokenizer.tokenize(post.lower()) for post in (df['joined'])]\n",
    "\n",
    "# Initializing lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# First had to lemmatize each word\n",
    "# then rejoin words into one string\n",
    "lems = []\n",
    "for post in tokens:\n",
    "    tok_post = []\n",
    "    for word in post:\n",
    "        tok_post.append(lemmatizer.lemmatize(word))\n",
    "    posts = \" \".join(tok_post)\n",
    "    lems.append(posts)\n",
    "\n",
    "# Adding the lemmatized data back to the DataFrame\n",
    "join['text'] = lems\n",
    "join['text'].astype(str)\n",
    "\n",
    "# Dropping unnecessary columns\n",
    "join=join.drop(['selftext','title','joined'],axis=1)\n",
    "join.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Adding in columns for word count. According to the EDA, I believe that this column will help add signal to the model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "join['word_count'] = join['text'].map(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>husband wa screwing his secretary up the as wh...</td>\n",
       "      <td>153</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>why doe batman wear dark clothing batman doesn...</td>\n",
       "      <td>132</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>a man is in court the judge say on the 3rd aug...</td>\n",
       "      <td>1156</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>a poor old lady wa forced to sell her valuable...</td>\n",
       "      <td>1540</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>how do you get a nun pregnant dress her up a a...</td>\n",
       "      <td>56</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                               text  char_count  \\\n",
       "0          0  husband wa screwing his secretary up the as wh...         153   \n",
       "1          0  why doe batman wear dark clothing batman doesn...         132   \n",
       "2          0  a man is in court the judge say on the 3rd aug...        1156   \n",
       "3          0  a poor old lady wa forced to sell her valuable...        1540   \n",
       "4          0  how do you get a nun pregnant dress her up a a...          56   \n",
       "\n",
       "   word_count  \n",
       "0          34  \n",
       "1          26  \n",
       "2         235  \n",
       "3         299  \n",
       "4          14  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test-Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = join[['text','word_count']]\n",
    "y = join['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1293, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TFIDF to Vectorize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initializing the TFIDF Vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words=stopWords, min_df=1, max_df=1.0)\n",
    "\n",
    "# Fitting and transforming the data\n",
    "X_train_tf = tfidf.fit_transform(X_train['text'])\n",
    "X_test_tf = tfidf.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1293, 5523)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SVD for Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf_df = pd.DataFrame(X_train_tf.todense(), columns=tfidf.get_feature_names())\n",
    "X_test_tf_df = pd.DataFrame(X_test_tf.todense(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the SVD model\n",
    "# From the LSA notebook, it was determined that 346 components \n",
    "# would allow for a variance explained score of 75%\n",
    "SVD = TruncatedSVD(n_components=346)\n",
    "X_train_svd = SVD.fit_transform(X_train_tf_df)\n",
    "X_test_svd = SVD.transform(X_test_tf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapped the SVD model into a DataFrame for the purpose of \n",
    "# adding back in the word_count and character_count features\n",
    "X_train_svd_df = pd.DataFrame(X_train_svd)\n",
    "X_test_svd_df = pd.DataFrame(X_test_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Had to reset the index of the original X_train and X_test data\n",
    "# Had trouble adding this data back into the SVD Dataframe \n",
    "# and resetting the index solved the issue\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding word_count back into the train and test data\n",
    "X_train_svd_df['word_count'] = X_train['word_count']\n",
    "X_test_svd_df['word_count'] = X_test['word_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Gradient Boosting is the process of iteratively improving a weak model by optimizing the model to predict the residuals (or errors). This is different from Adaptive Boosting, where iterative models put heavier weight on misclassified observations._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6411446249033256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 1, 'n_estimators': 50}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "params = {\n",
    "    'max_depth' : [1,2,3,4],\n",
    "    'n_estimators' : [25, 50, 75],\n",
    "}\n",
    "grid = GridSearchCV(gb, param_grid = params, cv=5)\n",
    "grid.fit(X_train_svd_df, y_train)\n",
    "print(grid.best_score_)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6481481481481481"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_test_svd_df, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_With GridSearch and Gradient Boosting, my baseline accuracy is now at about 65%. I decided to try to fit an Adaptive Boosting model to see if it increases my accuracy. Adaptive Boosting uses an iterative process and puts a heavier weight on observations that were misclassified in the iterations._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.617169373549884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 50}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(random_state=42)\n",
    "params = {\n",
    "    'n_estimators' : [40,50,60]\n",
    "}\n",
    "grid1 = GridSearchCV(ada, param_grid = params, cv=5)\n",
    "grid1.fit(X_train_svd_df, y_train)\n",
    "print(grid1.best_score_)\n",
    "grid1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5902777777777778"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid1.score(X_test_svd_df, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_It seems Adaptive Boosting modeling doesn't seem to improve the baseline accuracy in this case._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_I achieved the highest accuracy model with Gradient Boosting at 65%. Admittedly, this was not as high a score that I wanted to achieve. However, I realize that finding the subtle difference between a Joke and an AntiJoke is something that even we as humans have difficulty explaining or understanding at times. The 65% accuracy score tells me that there is definitely some signal that machine learning is picking up, which is a hopeful sign for future endeavors. Future steps would be to become more well-versed in the academic literature concerning AI and humor, as well as looking more deeply into NLP as a whole._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
