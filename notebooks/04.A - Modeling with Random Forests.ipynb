{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook will use Random Forests modeling to classify the Reddit data into the correct subreddit. First, the joined data will be split into train and test data, then preprocessed with Latent Semantic Analysis for dimensionality reduction, and then have the Random Forests model applied._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining, Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1725, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the dataset\n",
    "join = pd.read_csv('./datasets/joined.csv')\n",
    "join.head()\n",
    "\n",
    "# Assigning target\n",
    "target = join['subreddit']\n",
    "\n",
    "# Dropping selftext and title columns\n",
    "df = join.drop(['selftext','title'], axis=1)\n",
    "\n",
    "# Importing nltk's English stop words\n",
    "# Note: Using CamelCase to prevent overwriting variable later\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "# From the EDA, I concluded that the following words were very common \n",
    "# in both /r/Jokes and /r/AntiJokes and were added noise. \n",
    "# I added these words to the nltk stop words\n",
    "stopWords.extend(['wa', 'say', 'said', 'did', 'like', 'asked', 'woman', \n",
    "                  'don', 'know', 'year', 'wife', 'good', 'want', 'got', \n",
    "                  'ha', 'people', 'make', 'tell', 'didn', 'joke', 'x200b', \n",
    "                  'way', 'think', 'walk', 'll', 'home', 't'])\n",
    "\n",
    "# Tokenizing by alphanumeric characters\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "\n",
    "# Making all tokens lowercase\n",
    "tokens = [tokenizer.tokenize(post.lower()) for post in (df['joined'])]\n",
    "\n",
    "# Initializing lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# First had to lemmatize each word\n",
    "# then rejoin words into one string\n",
    "lems = []\n",
    "for post in tokens:\n",
    "    tok_post = []\n",
    "    for word in post:\n",
    "        tok_post.append(lemmatizer.lemmatize(word))\n",
    "    posts = \" \".join(tok_post)\n",
    "    lems.append(posts)\n",
    "\n",
    "# Adding the lemmatized data back to the DataFrame\n",
    "join['text'] = lems\n",
    "join['text'].astype(str)\n",
    "\n",
    "# Dropping unnecessary columns\n",
    "join=join.drop(['selftext','title','joined'],axis=1)\n",
    "join.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Adding in columns for word count and character count. According to the EDA, I believe that these columns will help add signal to the model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "join['char_count'] = join['text'].map(len)\n",
    "join['word_count'] = join['text'].map(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>husband wa screwing his secretary up the as wh...</td>\n",
       "      <td>153</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>why doe batman wear dark clothing batman doesn...</td>\n",
       "      <td>132</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>a man is in court the judge say on the 3rd aug...</td>\n",
       "      <td>1156</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>a poor old lady wa forced to sell her valuable...</td>\n",
       "      <td>1540</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>how do you get a nun pregnant dress her up a a...</td>\n",
       "      <td>56</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                               text  char_count  \\\n",
       "0          0  husband wa screwing his secretary up the as wh...         153   \n",
       "1          0  why doe batman wear dark clothing batman doesn...         132   \n",
       "2          0  a man is in court the judge say on the 3rd aug...        1156   \n",
       "3          0  a poor old lady wa forced to sell her valuable...        1540   \n",
       "4          0  how do you get a nun pregnant dress her up a a...          56   \n",
       "\n",
       "   word_count  \n",
       "0          34  \n",
       "1          26  \n",
       "2         235  \n",
       "3         299  \n",
       "4          14  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test-Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = join[['text','char_count','word_count']]\n",
    "y = join['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1293, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TFIDF to Vectorize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initializing the TFIDF Vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words=stopWords, min_df=1, max_df=1.0)\n",
    "\n",
    "# Fitting and transforming the data\n",
    "X_train_tf = tfidf.fit_transform(X_train['text'])\n",
    "X_test_tf = tfidf.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1293, 5523)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SVD for Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf_df = pd.DataFrame(X_train_tf.todense(), columns=tfidf.get_feature_names())\n",
    "X_test_tf_df = pd.DataFrame(X_test_tf.todense(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the SVD model\n",
    "# From the LSA notebook, it was determined that 346 components \n",
    "# would allow for a variance explained score of 75%\n",
    "SVD = TruncatedSVD(n_components=346)\n",
    "X_train_svd = SVD.fit_transform(X_train_tf_df)\n",
    "X_test_svd = SVD.transform(X_test_tf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapped the SVD model into a DataFrame for the purpose of \n",
    "# adding back in the word_count and character_count features\n",
    "X_train_svd_df = pd.DataFrame(X_train_svd)\n",
    "X_test_svd_df = pd.DataFrame(X_test_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Had to reset the index of the original X_train and X_test data\n",
    "# Had trouble adding this data back into the SVD Dataframe \n",
    "# and resetting the index solved the issue\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding word_count and char_count back into the train and test data\n",
    "X_train_svd_df['word_count'] = X_train['word_count']\n",
    "X_train_svd_df['char_count'] = X_train['char_count']\n",
    "X_test_svd_df['word_count'] = X_test['word_count']\n",
    "X_test_svd_df['char_count'] = X_test['char_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Random Forests Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Random Forest models take advantage of the aggregating nature of bagged decision tree models coupled with the factor of limited random features, forcing the model to make decisions based on a fraction of the available information. The aggregation results in a model that has low variance and high accuracy._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6767208043310131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': None,\n",
       " 'max_features': 1.0,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "params = {\n",
    "    'n_estimators' : [175, 200, 225],\n",
    "    'max_depth' : [None],\n",
    "    # Using 1.0 will use all features, quick way to test bagging classifier\n",
    "    'max_features' : ['auto', 1.0],\n",
    "    'min_samples_split' : [2,3]\n",
    "}\n",
    "grid = GridSearchCV(rf, param_grid = params, cv=5)\n",
    "grid.fit(X_train_svd_df, y_train)\n",
    "print(grid.best_score_)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_With GridSearch and Random Forests, my baseline accuracy is now at about 64%. I decided to try to fit an Extra Trees model to see if it increases my accuracy. Extra Trees model builds on top of the Random Forests model but has an additional factor of randomness (random values to divide the features, which are also randomly selected)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6790409899458624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': None,\n",
       " 'max_features': 1.0,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "et = ExtraTreesClassifier(random_state=42)\n",
    "params = {\n",
    "    'n_estimators' : [100, 150, 200],\n",
    "    'max_depth' : [None,3,4],\n",
    "    # Using 1.0 will use all features, quick way to test bagging classifier\n",
    "    'max_features' : ['auto', 1.0],\n",
    "    'min_samples_split' : [2,3,4]\n",
    "}\n",
    "grid1 = GridSearchCV(et, param_grid = params, cv=5)\n",
    "grid1.fit(X_train_svd_df, y_train)\n",
    "print(grid1.best_score_)\n",
    "grid1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6412037037037037"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid1.score(X_test_svd_df, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Extra Trees modeling doesn't seem to improve the baseline accuracy in this case, achieving roughly the same score._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In this notebook, I took the cleaned and preprocessed data and applied Random Forests and Extra Trees models to try to build a high accuracy model. The highest score achieved was about 64% accuracy. Next, I will apply Gradient Boosting to try to achieve a higher accuracy score in the next notebook._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
